# st_sample.py (v15: chat above input + scroll after input, context builder visible, new ollama defaults/options)
# -*- coding: utf-8 -*-

import os
import re
import json
import time
import traceback
from typing import Any, Dict, Optional, List, Tuple
from urllib.parse import quote_plus, urlencode
from datetime import datetime

import numpy as np
import pandas as pd
import streamlit as st
from streamlit import components
import plotly.express as px

# AgGrid
from st_aggrid.grid_options_builder import GridOptionsBuilder
from st_aggrid import AgGrid

# SQLAlchemy / pgvector
from sqlalchemy.sql import bindparam
from sqlalchemy import create_engine, text, event
import pgvector.sqlalchemy
try:
    from pgvector.psycopg import register_vector
except Exception:
    register_vector = None

# agno
from agno.agent import Agent
from agno.tools import tool
from agno.models.ollama import Ollama
try:
    from agno.models.openai import OpenAIChat
except Exception:
    OpenAIChat = None
from agno.embedder.openai import OpenAIEmbedder
from agno.embedder.ollama import OllamaEmbedder
from agno.vectordb.pgvector import PgVector, SearchType
from agno.knowledge.agent import AgentKnowledge


# ==================== Page Config & Styles ====================
st.set_page_config(page_title="í•œíˆ¬ í‡´ì§ë§ˆìŠ¤í„°", layout="wide")
st.markdown("""
<style>
/* tighter global padding */
.main .block-container { padding-top: .2rem; padding-left: .5rem; padding-right: .5rem; }
/* bigger centered title */
.center-title { text-align:center; margin: .25rem 0 .6rem 0; font-size: 2.25rem; font-weight: 800; letter-spacing: -0.02em; }
/* slimmer panels */
.panel-soft { padding: 10px 12px; border-radius: 12px; background: #ffffff;
  border: 1px solid rgba(0,0,0,.06); box-shadow: 0 1px 2px rgba(0,0,0,.03); }
.panel-soft.flush-top { padding-top: 0; }
.panel-soft > :first-child { margin-top: 0 !important; }
/* slimmer vertical separator */
.v-sep { border-left: 1px solid #e9ecef; height: calc(100vh - 140px); margin: 6px 4px; }
/* data summary */
.summary-card { border:1px solid #e9ecef; border-radius:12px; padding:10px 12px; }
.summary-card table { width:100%; font-size:14px; border-collapse:collapse; }
.summary-card td { padding:6px 4px; }
.small-note { color:#6c757d; font-size:12px; margin-top:4px; }
/* lightweight badge for hidden thoughts */
.badge-thinking { display:inline-block; padding:2px 8px; border-radius:999px; font-size:12px; background:#f1f3f5; color:#495057; margin:0 4px; }
</style>
<div class="center-title">í•œíˆ¬ í‡´ì§ë§ˆìŠ¤í„°</div>
""", unsafe_allow_html=True)


# ==================== Constants ====================
DEFAULT_PARAM_SCHEMA: Dict[str, Any] = {"notes": ""}
GRID_KEYS = {"acct": "grid_acct_v15", "dc": "grid_dc_v15"}

# ë¼ë²¨ ë§µ
KMAP_CUSTOMER = {"customer_id": "ê³ ê° ë²ˆí˜¸","customer_name": "ê³ ê° ì´ë¦„","brth_dt": "ìƒë…„ì›”ì¼","age_band": "ì—°ë ¹ëŒ€"}
KMAP_ACCOUNT  = {"account_id": "ê³„ì¢Œ ë²ˆí˜¸","customer_id": "ê³ ê° ë²ˆí˜¸","acnt_type": "ê³„ì¢Œ ìœ í˜•","prd_type_cd": "ìƒí’ˆì½”ë“œ","acnt_bgn_dt": "ê°œì„¤ì¼ì","acnt_evlu_amt": "í‰ê°€ì ë¦½ê¸ˆ"}
KMAP_DC       = {"ctrt_no": "ê³„ì•½ë²ˆí˜¸","odtp_name": "ê·¼ë¬´ì²˜ëª…","etco_dt": "ì…ì‚¬ì¼ì","midl_excc_dt": "ì¤‘ê°„ì •ì‚°ì¼ì","sst_join_dt": "ì œë„ê°€ì…ì¼ì","almt_pymt_prca": "ë¶€ë‹´ê¸ˆë‚©ì…ì›ê¸ˆ","utlz_pfls_amt": "ìš´ìš©ì†ìµê¸ˆì•¡","evlu_acca_smtl_amt": "í‰ê°€ì ë¦½ê¸ˆí•©ê³„ê¸ˆì•¡"}


# ==================== JSON Utils ====================
def _json_default(obj):
    if isinstance(obj, (np.integer,)): return int(obj)
    if isinstance(obj, (np.floating,)): return float(obj)
    if isinstance(obj, (np.ndarray,)): return obj.tolist()
    return str(obj)

def to_json_str(data: dict) -> str:
    return json.dumps(data, ensure_ascii=False, indent=2, default=_json_default)

def koreanize_dict(d: Optional[Dict[str, Any]], kmap: Dict[str, str]) -> Optional[Dict[str, Any]]:
    if d is None: return None
    if isinstance(d, dict) and len(d) == 0: return {}
    return {kmap.get(k, k): v for k, v in d.items()}

def koreanize_payload(payload: Dict[str, Any]) -> Dict[str, Any]:
    ctx = payload.get("context")
    ctx = ctx if isinstance(ctx, dict) else {}
    cust = ctx.get("customer")
    accts = ctx.get("accounts") or []
    dcs = ctx.get("dc_contracts") or []
    return {
        "íŒŒë¼ë¯¸í„°": payload.get("params"),
        "ì»¨í…ìŠ¤íŠ¸": {} if len(ctx) == 0 else {
            "ê³ ê°": koreanize_dict(cust, KMAP_CUSTOMER),
            "ê³„ì¢Œë“¤": [koreanize_dict(a, {**KMAP_ACCOUNT, "_account_id": "_account_id"}) for a in accts],
            "DC ê³„ì•½": [koreanize_dict(x, KMAP_DC) for x in dcs],
        },
    }


# ==================== Secrets/DB Helpers ====================
def _safe_secret(name: str, default: Optional[str] = None) -> Optional[str]:
    try:
        if name in st.secrets:
            return st.secrets[name]
    except Exception:
        pass
    return os.getenv(name, default)

def _safe_schema(schema: Optional[str]) -> str:
    if not schema: return "public"
    schema = schema.strip()
    return schema if re.fullmatch(r"[A-Za-z0-9_]+", schema) else "public"

def _get_pg_conn_str() -> str:
    host = _safe_secret("PG_HOST", "localhost") or "localhost"
    port = _safe_secret("PG_PORT", "5432") or "5432"
    db   = _safe_secret("PG_DB", "postgres") or "postgres"
    user = _safe_secret("PG_USER", "postgres") or "postgres"
    pwd  = _safe_secret("PG_PASSWORD", None)
    if not pwd:
        raise RuntimeError("PG_PASSWORDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    schema = _safe_schema(_safe_secret("PG_SCHEMA", "public"))
    user_q = quote_plus(user); pwd_q = quote_plus(pwd)
    host_q = quote_plus(host); db_q = quote_plus(db)
    query = urlencode({"options": f"-csearch_path={schema}"})
    return f"postgresql+psycopg://{user_q}:{pwd_q}@{host_q}:{port}/{db_q}?{query}"

def _make_engine_with_schema():
    eng = create_engine(_get_pg_conn_str(), pool_pre_ping=True)
    @event.listens_for(eng, "connect")
    def _on_connect(dbapi_connection, connection_record):
        if register_vector is not None:
            try: register_vector(dbapi_connection)
            except Exception: pass
    return eng


# ==================== DB Loaders ====================
@st.cache_data(ttl=60)
def load_customers_from_db() -> pd.DataFrame:
    engine = _make_engine_with_schema()
    sql = text("""SELECT customer_id, customer_name, brth_dt, age_band FROM kis_customers ORDER BY customer_id""")
    with engine.begin() as conn:
        df = pd.read_sql(sql, conn)
    df["brth_dt"] = pd.to_datetime(df["brth_dt"], errors="coerce").dt.date
    df.rename(columns={"customer_id":"ê³ ê° ë²ˆí˜¸","customer_name":"ê³ ê° ì´ë¦„","brth_dt":"ìƒë…„ì›”ì¼","age_band":"ì—°ë ¹ëŒ€"}, inplace=True)
    df["_customer_id"] = df["ê³ ê° ë²ˆí˜¸"]
    return df

@st.cache_data(ttl=60)
def load_accounts_from_db(customer_filter: Optional[Any] = None) -> pd.DataFrame:
    engine = _make_engine_with_schema()
    base_sql = """SELECT account_id, customer_id, acnt_type, prd_type_cd, acnt_bgn_dt, acnt_evlu_amt FROM kis_accounts"""
    params: Dict[str, Any] = {}
    if customer_filter is None:
        stmt = text(base_sql + " ORDER BY account_id")
    else:
        if isinstance(customer_filter, (list, tuple, set)):
            stmt = text(base_sql + " WHERE customer_id IN :cids ORDER BY account_id").bindparams(bindparam("cids", expanding=True))
            params["cids"] = list(customer_filter)
        else:
            stmt = text(base_sql + " WHERE customer_id = :cid ORDER BY account_id")
            params["cid"] = customer_filter

    with engine.begin() as conn:
        df = pd.read_sql(stmt, conn, params=params)

    df["acnt_bgn_dt"] = pd.to_datetime(df["acnt_bgn_dt"], errors="coerce").dt.date
    df["acnt_evlu_amt"] = pd.to_numeric(df["acnt_evlu_amt"], errors="coerce")
    df.rename(columns={
        "account_id":"ê³„ì¢Œ ë²ˆí˜¸","customer_id":"ê³ ê° ë²ˆí˜¸","acnt_type":"ê³„ì¢Œ ìœ í˜•",
        "prd_type_cd":"ìƒí’ˆì½”ë“œ","acnt_bgn_dt":"ê°œì„¤ì¼ì","acnt_evlu_amt":"í‰ê°€ì ë¦½ê¸ˆ"
    }, inplace=True)
    df["_account_id"] = df["ê³„ì¢Œ ë²ˆí˜¸"].astype(str)
    df["_customer_id"] = df["ê³ ê° ë²ˆí˜¸"]
    return df

@st.cache_data(ttl=60)
def load_dc_contracts_from_db(account_filter: Optional[Any] = None) -> pd.DataFrame:
    engine = _make_engine_with_schema()
    schema = _safe_schema(_safe_secret("PG_SCHEMA", "public"))
    base_sql = f"""
      SELECT ctrt_no, odtp_name, etco_dt, midl_excc_dt, sst_join_dt, almt_pymt_prca, utlz_pfls_amt, evlu_acca_smtl_amt
      FROM {schema}.kis_dc_contract
    """
    params: Dict[str, Any] = {}
    if account_filter is None or (isinstance(account_filter, list) and len(account_filter) == 0):
        return pd.DataFrame(columns=["ê³„ì•½ë²ˆí˜¸","ê·¼ë¬´ì²˜ëª…","ì…ì‚¬ì¼ì","ì¤‘ê°„ì •ì‚°ì¼ì","ì œë„ê°€ì…ì¼ì","ë¶€ë‹´ê¸ˆë‚©ì…ì›ê¸ˆ","ìš´ìš©ì†ìµê¸ˆì•¡","í‰ê°€ì ë¦½ê¸ˆí•©ê³„ê¸ˆì•¡","_ctrt_no"])
    else:
        if isinstance(account_filter, (list, tuple, set)):
            stmt = text(base_sql + " WHERE ctrt_no IN :ids ORDER BY ctrt_no").bindparams(bindparam("ids", expanding=True))
            params["ids"] = [str(x) for x in list(account_filter)]
        else:
            stmt = text(base_sql + " WHERE ctrt_no = :id ORDER BY ctrt_no")
            params["id"] = str(account_filter)

    with engine.begin() as conn:
        df = pd.read_sql(stmt, conn, params=params)

    for col in ["etco_dt","midl_excc_dt","sst_join_dt"]:
        df[col] = pd.to_datetime(df[col], errors="coerce").dt.date
    for col in ["almt_pymt_prca","utlz_pfls_amt","evlu_acca_smtl_amt"]:
        df[col] = pd.to_numeric(df[col], errors="coerce")

    df.rename(columns={
        "ctrt_no": "ê³„ì•½ë²ˆí˜¸","odtp_name": "ê·¼ë¬´ì²˜ëª…","etco_dt": "ì…ì‚¬ì¼ì","midl_excc_dt": "ì¤‘ê°„ì •ì‚°ì¼ì",
        "sst_join_dt": "ì œë„ê°€ì…ì¼ì","almt_pymt_prca": "ë¶€ë‹´ê¸ˆë‚©ì…ì›ê¸ˆ","utlz_pfls_amt": "ìš´ìš©ì†ìµê¸ˆì•¡","evlu_acca_smtl_amt": "í‰ê°€ì ë¦½ê¸ˆí•©ê³„ê¸ˆì•¡",
    }, inplace=True)
    df["_ctrt_no"] = df["ê³„ì•½ë²ˆí˜¸"].astype(str)
    return df


# ==================== AgGrid Helper ====================
def aggrid_table(
    df: pd.DataFrame, key: str, selection_mode="none", height=280,
    enable_filter=True, fit_columns_on_load=True, allow_horizontal_scroll=False
):
    gob = GridOptionsBuilder.from_dataframe(df)
    gob.configure_default_column(sortable=True, resizable=True, filter=enable_filter, wrapText=False, autoHeight=False)
    if selection_mode in ("single", "multiple"):
        gob.configure_selection(selection_mode=selection_mode, use_checkbox=(selection_mode=="multiple"))
    grid_options = gob.build()
    grid_options["suppressHorizontalScroll"] = not allow_horizontal_scroll

    # Prefer new API: update_on accepts grid events, fall back to deprecated update_mode if needed.
    try:
        return AgGrid(
            df, gridOptions=grid_options, update_on=["filterChanged", "modelUpdated"],
            height=height, key=key,
            fit_columns_on_grid_load=bool(fit_columns_on_load), allow_unsafe_jscode=True, enable_enterprise_modules=False,
        )
    except TypeError:
        # Fallback for older st-aggrid
        from st_aggrid import GridUpdateMode  # lazy import to avoid global deprecation warning
        update_mode = GridUpdateMode.MODEL_CHANGED | GridUpdateMode.FILTERING_CHANGED
        return AgGrid(
            df, gridOptions=grid_options, update_mode=update_mode,
            height=height, key=key,
            fit_columns_on_grid_load=bool(fit_columns_on_load), allow_unsafe_jscode=True, enable_enterprise_modules=False,
        )


# ==================== Context Builders ====================
def _rows_to_accounts(rows: pd.DataFrame) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    if rows is None or rows.empty: 
        return out
    for _, r in rows.iterrows():
        out.append({
            "account_id": str(r["_account_id"]),
            "customer_id": r["_customer_id"],
            "acnt_type": r["ê³„ì¢Œ ìœ í˜•"],
            "prd_type_cd": r["ìƒí’ˆì½”ë“œ"],
            "acnt_bgn_dt": str(r["ê°œì„¤ì¼ì"]),
            "acnt_evlu_amt": int(pd.to_numeric(r["í‰ê°€ì ë¦½ê¸ˆ"], errors="coerce") or 0),
        })
    return out

def _rows_to_dc_list(rows: pd.DataFrame) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    if rows is None or rows.empty: return out
    for _, r in rows.iterrows():
        out.append({
            "ctrt_no": str(r["_ctrt_no"]),
            "odtp_name": r["ê·¼ë¬´ì²˜ëª…"],
            "etco_dt": str(r["ì…ì‚¬ì¼ì"]),
            "midl_excc_dt": str(r["ì¤‘ê°„ì •ì‚°ì¼ì"]) if pd.notna(r["ì¤‘ê°„ì •ì‚°ì¼ì"]) else None,
            "sst_join_dt": str(r["ì œë„ê°€ì…ì¼ì"]),
            "almt_pymt_prca": int(pd.to_numeric(r["ë¶€ë‹´ê¸ˆë‚©ì…ì›ê¸ˆ"], errors="coerce") or 0),
            "utlz_pfls_amt": int(pd.to_numeric(r["ìš´ìš©ì†ìµê¸ˆì•¡"], errors="coerce") or 0),
            "evlu_acca_smtl_amt": int(pd.to_numeric(r["í‰ê°€ì ë¦½ê¸ˆí•©ê³„ê¸ˆì•¡"], errors="coerce") or 0),
        })
    return out

def build_context_from_selection() -> Dict[str, Any]:
    selected_customer: Optional[str] = st.session_state.get("selected_customer")
    df_acct: pd.DataFrame = st.session_state.get("df_acct", pd.DataFrame())
    df_dc: pd.DataFrame = st.session_state.get("df_dc", pd.DataFrame())

    cust = None
    if selected_customer:
        row = st.session_state.df_cust.query("_customer_id == @selected_customer")
        if not row.empty:
            r = row.iloc[0]
            cust = {"customer_id": r["_customer_id"], "customer_name": r["ê³ ê° ì´ë¦„"],
                    "brth_dt": str(r["ìƒë…„ì›”ì¼"]), "age_band": r["ì—°ë ¹ëŒ€"]}

    accts = _rows_to_accounts(df_acct)
    dc_list = _rows_to_dc_list(df_dc)

    return {"customer": cust, "accounts": accts, "dc_contracts": dc_list}

def build_context_for_chat() -> Dict[str, Any]:
    ctx = st.session_state.get("context")
    return ctx if isinstance(ctx, dict) else {}


# ==================== Dummy Simulator / Agent Factory ====================
@tool
def run_pension_simulator(params: dict) -> dict:
    return {"source": "dummy","as_of": datetime.now().strftime("%Y-%m-%d %H:%M"),"echo_params": params,"message": "ìƒ˜í”Œ ë”ë¯¸ ì‘ë‹µì…ë‹ˆë‹¤."}

def make_knowledge_base(provider) -> AgentKnowledge:
    env_name = 'AGNO_OPENAI_KG_TABLE' if provider == 'openai' else 'AGNO_OLLAMA_KG_TABLE'

    table = os.getenv(env_name, "kis_pension_knowledge")
    embedder = OpenAIEmbedder() if provider == "openai" else OllamaEmbedder(id="openhermes")

    search = (os.getenv("AGNO_KG_SEARCH", "hybrid") or "hybrid").lower()
    search_type = SearchType.hybrid if search == "hybrid" else (SearchType.fulltext if search == "fulltext" else SearchType.vector)
    engine = _make_engine_with_schema()
    
    vector_db = PgVector(db_engine=engine, table_name=table, embedder=embedder, search_type=search_type)
    class VectorOnlyKnowledge(AgentKnowledge):
        def __init__(self, vector_db, filters=None, name: str = "kis_pension_knowledge"):
            super().__init__(vector_db=vector_db, filters=filters, name=name)
        @property
        def document_lists(self): return []
        def load(self): return self
    return VectorOnlyKnowledge(vector_db=vector_db, name=table)

def make_agent(provider: str, model_id: str, req: Dict[str, Any], search_knowledge: bool) -> Agent:
    sys = """
        ë‹¹ì‹ ì€ í‡´ì§ì—°ê¸ˆ ìƒë‹´ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ë©°, ì•„ë˜ ì›ì¹™ì„ ë”°ë¦…ë‹ˆë‹¤.

        [ì˜ë„ íŒŒì•…]
        - ì§ˆë¬¸ì„ (A) ì»¨í…ìŠ¤íŠ¸ ì§ˆì˜(ê³ ê°/ê³„ì¢Œ/ê³„ì•½ ìƒíƒœÂ·ê³„ì‚°), (B) ì •ì±…Â·ìš©ì–´ ì§ˆì˜(ê·œì •Â·ì ˆì°¨Â·FAQ), (C) ì‹¤í–‰ ìš”ì²­(ì‹œë®¬ë ˆì´ì…˜Â·ë¹„êµ) ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
        - í˜¼í•©ì¼ ë•ŒëŠ” í•µì‹¬ ì˜ë„ 1ê°œë¥¼ ìš°ì„  ì²˜ë¦¬í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” â€œë‹¤ìŒ ë‹¨ê³„â€ë¡œ ì œì•ˆí•©ë‹ˆë‹¤.

        [ì •ë³´ ì¶œì²˜ ìš°ì„ ìˆœìœ„ â€” RAG ìµœìš°ì„ ]
        1) Knowledge/RAG(FAQ í¬í•¨): ì •ì±…Â·ìš©ì–´Â·ì ˆì°¨ ê´€ë ¨ ë‹µë³€ì€ ë°˜ë“œì‹œ RAG ê·¼ê±°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤(FAQ ìš°ì„ ).
        2) ì»¨í…ìŠ¤íŠ¸(JSON): ê³ ê°Â·ê³„ì¢ŒÂ·DC ê³„ì•½(DCëŠ” ctrt_no=account_id ë§¤í•‘) ì¡°ê±´ìœ¼ë¡œ ê°œì¸í™”í•©ë‹ˆë‹¤.
        3) ì¶”ì • ê¸ˆì§€: ì»¨í…ìŠ¤íŠ¸/ì§€ì‹ì— ì—†ìœ¼ë©´ ë¶€ì¡±í•¨ì„ ëª…ì‹œí•˜ê³ , í•„ìš”í•œ ê²½ìš° ë³´ì¶© ì§ˆë¬¸ 1ê°œë§Œ í•©ë‹ˆë‹¤.

        [ë§ì¶¤í˜• ì‘ë‹µ]
        - ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜ì˜í•œ ì¡°ê±´(ê³„ì¢Œìœ í˜•, ê¸ˆì•¡, ë‚ ì§œ, ê³„ì•½)ì„ ëª…ì‹œì ìœ¼ë¡œ í‘œê¸°í•©ë‹ˆë‹¤.
        - ìˆ«ìëŠ” 1,234,567 í˜•ì‹, ë‚ ì§œëŠ” YYYY-MM-DD í˜•ì‹ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

        [ë‹µë³€ í˜•ì‹]
        - ìš”ì•½(3ì¤„ ì´ë‚´) â†’ ì„¤ëª…/ê·¼ê±°(ë¶ˆë¦¿Â·í‘œ) â†’ ìœ ì˜ì‚¬í•­ â†’ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ.
        - ê·¼ê±°ì—ëŠ” ì§€ì‹ ë¬¸ì„œì˜ ì œëª©/ì„¹ì…˜/ê°œì •ì¼ë§Œ ì§§ê²Œ í‘œê¸°(ë‚´ë¶€ IDÂ·ê²½ë¡œÂ·ë¡œê·¸ ë…¸ì¶œ ê¸ˆì§€).
        - ìš”ì²­ ì‹œì—ë§Œ JSONì„ ë³´ì—¬ì£¼ë©°, ë‚´ë¶€ í‚¤/IDëŠ” ìˆ¨ê¹ë‹ˆë‹¤.

        [ë„êµ¬/ì‹œë®¬ë ˆì´ì…˜]
        - ê³„ì‚°/ë¹„êµ/ì‹œë‚˜ë¦¬ì˜¤ê°€ í•„ìš”í•˜ë©´ run_pension_simulator ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ê³  í•µì‹¬ ê²°ê³¼ë§Œ ìš”ì•½í•©ë‹ˆë‹¤.

        [ë¶ˆí™•ì‹¤ì„±]
        - ì•„ëŠ” ì‚¬ì‹¤ê³¼ ê°€ì •ì„ ë¶„ë¦¬í•˜ê³ , ì¶”ê°€ë¡œ í•„ìš”í•œ ì •ë³´ 1ê°œë§Œ êµ¬ì²´ì ìœ¼ë¡œ ìš”ì²­í•œ í›„ ê°€ëŠ¥í•œ ë²”ìœ„ì˜ ìµœì„  ë‹µë³€ì„ ì œì‹œí•©ë‹ˆë‹¤.

        [ê¸ˆì§€Â·í†¤]
        - ìƒê° ê³¼ì •(COT)Â·ë‚´ë¶€ ì²´ê³„ ë…¸ì¶œ ê¸ˆì§€. ê¸ˆìœµÂ·ì„¸ë²• í•´ì„ì€ ì¼ë°˜ ì•ˆë‚´ë¡œ ì œí•œ, í•„ìš” ì‹œ ì „ë¬¸ê°€ ìƒë‹´ ê¶Œê³ ë¥¼ ì§§ê²Œ ë§ë¶™ì…ë‹ˆë‹¤.
        - ì „ë¬¸ì ì´ë˜ ì¹œì ˆÂ·ê°„ê²°í•œ í†¤ì„ ìœ ì§€í•©ë‹ˆë‹¤.

        [ê¶Œì¥ ì¶œë ¥ í”„ë ˆì„]
        - ìš”ì•½: 2â€“3ë¬¸ì¥
        - ê³ ê°/ê³„ì¢Œ ê¸°ì¤€ ì„¤ëª…: ë¶ˆë¦¿ 2â€“5ê°œ
        - ê·œì •/ê·¼ê±°(ìš”ì•½): ë¶ˆë¦¿ 2â€“4ê°œ (ë¬¸ì„œëª…/ì„¹ì…˜/ê°œì •ì¼)
        - ë‹¤ìŒ ë‹¨ê³„: 1â€“2ê°œ (ì˜ˆ: â€œì¤‘ê°„ì •ì‚° ìê²© í™•ì¸â€, â€œì‹œë®¬ë ˆì´í„° ì‹¤í–‰â€)

    """
    if provider == "openai":
        if OpenAIChat is None:
            raise RuntimeError("OpenAIChat ëª¨ë¸ì´ í˜„ì¬ agno ë²„ì „ì— ì—†ìŠµë‹ˆë‹¤. agno ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        api_key = _safe_secret("OPENAI_API_KEY", None)
        if not api_key:
            raise RuntimeError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # âœ… OpenAIëŠ” max_tokens < 1 í—ˆìš© ì•ˆ í•¨ â†’ ì•ˆì „í•˜ê²Œ ë³´ì •
        opts = req.get("options", {}) or {}
        temperature = float(opts.get("temperature", 0.3))
        top_p = float(opts.get("top_p", 0.9))
        max_tokens_raw = opts.get("num_predict", 1024)
        try:
            max_tokens = int(max_tokens_raw)
        except Exception:
            max_tokens = 1024
        if max_tokens < 1:
            max_tokens = 1024  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë³´ì •

        model = OpenAIChat(
            id=model_id,
            api_key=api_key,
            request_params={
                "temperature": temperature,
                "top_p": top_p,
                "max_tokens": max_tokens,
            },
        )
    else:
        # Ollama: ëª¨ë“  ì„¸ë¶€ ì˜µì…˜ì€ 'options'ë¡œ ì „ë‹¬
        model = Ollama(id=model_id, request_params=req)

    return Agent(
        system_message=sys,
        model=model,
        tools=[run_pension_simulator],
        markdown=True,
        knowledge=make_knowledge_base(provider),
        search_knowledge=search_knowledge,
        enable_agentic_knowledge_filters=True,
        show_tool_calls=True,
        debug_mode=False,
    )


def _normalize_llm_cfg(raw: Dict[str, Any]) -> Tuple[str, str, Dict[str, Any], bool]:
    """provider, model_id, request_params, search_knowledge"""
    provider = raw.get("provider", "ollama")
    model_id = raw.get("model_id", "qwen3-h2o2-14b")
    # í†µì¼ëœ options dictë¡œ ì •ë¦¬
    options = raw.get("options") or {}
    # ê¸°ë³¸ê°’ ë³´ê°•
    opts = {
        "temperature": float(options.get("temperature", 0.3)),
        "top_p": float(options.get("top_p", 0.9)),
        "top_k": int(options.get("top_k", 40)),
        "repeat_penalty": float(options.get("repeat_penalty", 1.05)),
        "num_ctx": int(options.get("num_ctx", 16384)),
        "num_predict": int(options.get("num_predict", -1)),
        "num_batch": int(options.get("num_batch", 512)),
        "num_thread": int(options.get("num_thread", 0)),
    }
    keep_alive = raw.get("keep_alive", "2h")
    req = {"keep_alive": keep_alive, "options": opts}
    return provider, model_id, req, bool(raw.get("search_knowledge", True))

def get_agent():
    cfg = st.session_state.get("agent_cfg") or {
        "provider":"ollama","model_id":"qwen3-h2o2-14b",
        "options":{"temperature":0.3,"top_p":0.9,"top_k":40,"repeat_penalty":1.05,"num_ctx":16384,"num_predict":-1,"num_batch":512,"num_thread":0},
        "keep_alive":"2h","search_knowledge":True
    }
    provider, model_id, req, search_knowledge = _normalize_llm_cfg(cfg)
    sig = (provider, model_id, json.dumps(req, sort_keys=True), search_knowledge)
    if "AGENT" not in st.session_state or st.session_state.get("_agent_sig") != sig:
        st.session_state.AGENT = make_agent(provider, model_id, req, search_knowledge)
        st.session_state._agent_sig = sig
    return st.session_state.AGENT

def context_as_text(ctx: Dict[str, Any]) -> str:
    return "### ì„ íƒì»¨í…ìŠ¤íŠ¸\n" + to_json_str(ctx)

# ====== Think Masking ======
def mask_thoughts(text: str, notice_inserted: bool) -> Tuple[str, bool]:
    t = text

    # 1) XML-style <think>...</think>
    pat_xml = re.compile(r"(?is)<\s*think\s*>.*?<\s*/\s*think\s*>")
    if pat_xml.search(t):
        if not notice_inserted:
            t = pat_xml.sub(" <span class='badge-thinking'>ìƒê° ì¤‘â€¦</span> ", t, count=1)
            notice_inserted = True
        t = pat_xml.sub("", t)

    # 2) Fenced code blocks ```think/analysis/...```
    pat_fence = re.compile(r"(?is)```(?:\s*(?:think|thoughts|analysis|chain[_ -]?of[_ -]?thought)[^\n]*)\n.*?```")
    if pat_fence.search(t):
        if not notice_inserted:
            t = pat_fence.sub(" <span class='badge-thinking'>ìƒê° ì¤‘â€¦</span> ", t, count=1)
            notice_inserted = True
        t = pat_fence.sub("", t)

    # 3) Bracketed tokens [think]...[/think] or ã€Thinkingã€‘â€¦
    pat_br = re.compile(r"(?is)[\[\{ï¼ˆ(ã€]\s*(?:think|thinking|thoughts)\s*[\]\}ï¼‰)ã€‘].*?[\[\{ï¼ˆ(ã€]\s*/?\s*(?:think|thinking|thoughts)\s*[\]\}ï¼‰)ã€‘]")
    if pat_br.search(t):
        if not notice_inserted:
            t = pat_br.sub(" <span class='badge-thinking'>ìƒê° ì¤‘â€¦</span> ", t, count=1)
            notice_inserted = True
        t = pat_br.sub("", t)

    # 4) Streaming partial start tokens: "...<think>" without close, or "think:" prefix at top
    lower = t.lower()
    s = lower.rfind("<think>")
    e = lower.rfind("</think>")
    if s != -1 and (e == -1 or e < s):
        t = t[:s]
        if not notice_inserted:
            t += " <span class='badge-thinking'>ìƒê° ì¤‘â€¦</span> "
            notice_inserted = True

    # 5) If text begins with "think: ..." lines before a blank line, strip them
    m = re.match(r"(?is)^\s*(?:think|analysis|thoughts)\s*[:>].*?(?:\n\s*\n|$)", t)
    if m:
        t = t[m.end():]
        if not notice_inserted:
            t = " <span class='badge-thinking'>ìƒê° ì¤‘â€¦</span> " + t
            notice_inserted = True

    return t, notice_inserted

def run_agent_stream(user_text: str, ctx: Dict[str, Any], debug: bool = False):
    agent = get_agent()
    full_prompt = f"{user_text}\n\n{context_as_text(ctx)}"
    st.session_state.last_debug = {"events": [], "error": None, "timing": {}}
    t0 = time.perf_counter()
    first_token_time = None
    try: agent.debug_mode = bool(debug)
    except Exception: pass
    try:
        for ev in agent.run(full_prompt, stream=True):
            content = getattr(ev, "content", None)
            event_name = getattr(ev, "event", "")
            if content and (event_name == "RunResponseContent" or isinstance(content, str)):
                if first_token_time is None:
                    first_token_time = time.perf_counter()
                    st.session_state.last_debug["timing"]["ttft_sec"] = round(first_token_time - t0, 3)
                yield content
        t1 = time.perf_counter()
        st.session_state.last_debug["timing"]["total_sec"] = round(t1 - t0, 3)
        if first_token_time is not None:
            st.session_state.last_debug["timing"]["stream_sec"] = round(t1 - first_token_time, 3)
    except Exception as e:
        st.session_state.last_debug["error"] = {"message": str(e), "traceback": traceback.format_exc()}
        yield f"\n\n[ì—ëŸ¬] {e}"


# ==================== Session Init ====================
st.session_state.setdefault("messages", [])
st.session_state.setdefault("last_debug", {"events": [], "error": None, "timing": {}})
st.session_state.setdefault("sim_params", DEFAULT_PARAM_SCHEMA.copy())
st.session_state.setdefault("context", {"customer": None, "accounts": [], "dc_contracts": []})
st.session_state.setdefault("selected_customer", None)
st.session_state.setdefault("agent_cfg", {
    "provider":"ollama","model_id":"qwen3-h2o2-14b",
    "options":{"temperature":0.3,"top_p":0.9,"top_k":40,"repeat_penalty":1.05,"num_ctx":16384,"num_predict":-1,"num_batch":512,"num_thread":0},
    "keep_alive":"2h","search_knowledge": True
})


# ==================== Layout ====================
left, midsep, right = st.columns([0.48, 0.02, 0.50])

# -------- LEFT --------
with left:
    st.markdown('<div class="panel-soft flush-top">', unsafe_allow_html=True)
    st.subheader("ê³ ê°/ê³„ì¢Œ ì •ë³´")

    # ê³ ê° ì„ íƒ (selectbox)
    if "df_cust" not in st.session_state:
        st.session_state.df_cust = load_customers_from_db()
    df_cust = st.session_state.df_cust
    all_names = df_cust["ê³ ê° ì´ë¦„"].dropna().astype(str).tolist()
    name_selected = st.selectbox("ê³ ê° ì´ë¦„ì„ ì„ íƒí•˜ì„¸ìš”. ê²€ìƒ‰ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤",
                                 options=[""] + sorted(all_names), index=0, key="customer_select")

    # ê³ ê°/ê³„ì¢Œ ë¡œë”©
    if name_selected:
        filtered_cust = df_cust[df_cust["ê³ ê° ì´ë¦„"] == name_selected]
    else:
        filtered_cust = df_cust.iloc[0:0]

    if not filtered_cust.empty:
        r = filtered_cust.iloc[0]
        newly_selected_cust = r["_customer_id"]

        if newly_selected_cust != st.session_state.get("selected_customer"):
            st.session_state.selected_customer = newly_selected_cust

            # 1) í•´ë‹¹ ê³ ê°ì˜ ëª¨ë“  ê³„ì¢Œ ë¡œë“œ
            st.session_state.df_acct = load_accounts_from_db(newly_selected_cust)

            # 2) DC ê³„ì•½ ì¬ì¡°íšŒ
            df_acct_all = st.session_state.df_acct
            dc_acct_ids = df_acct_all.loc[
                (df_acct_all["ê³„ì¢Œ ìœ í˜•"] == "DC") & (df_acct_all["_account_id"].notna()),
                "_account_id"
            ].astype(str).tolist()
            st.session_state.df_dc = load_dc_contracts_from_db(dc_acct_ids)

            # 3) ì»¨í…ìŠ¤íŠ¸ ì¬ìƒì„±
            st.session_state.context = build_context_from_selection()
            st.rerun()

        # ê³ ê° ìš”ì•½ ì¹´ë“œ
        st.markdown(f"""
        <div class="summary-card">
          <table>
            <tr><td style="color:#6c757d;width:110px;">ê³ ê° ë²ˆí˜¸</td><td><b>{r["ê³ ê° ë²ˆí˜¸"]}</b></td></tr>
            <tr><td style="color:#6c757d;">ê³ ê° ì´ë¦„</td><td>{r["ê³ ê° ì´ë¦„"]}</td></tr>
            <tr><td style="color:#6c757d;">ìƒë…„ì›”ì¼</td><td>{r["ìƒë…„ì›”ì¼"]}</td></tr>
            <tr><td style="color:#6c757d;">ì—°ë ¹ëŒ€</td><td>{r["ì—°ë ¹ëŒ€"]}</td></tr>
          </table>
        </div>
        """, unsafe_allow_html=True)

    else:
        if st.session_state.get("selected_customer") is not None:
            st.session_state.selected_customer = None
            st.session_state.df_acct = pd.DataFrame()
            st.session_state.df_dc = pd.DataFrame()
            st.session_state.context = {"customer": None, "accounts": [], "dc_contracts": []}
            st.rerun()
        st.info("ê³ ê°ì„ ì„ íƒí•˜ì„¸ìš”.")

    st.markdown("---")

    # â‘¡ ê³„ì¢Œ ì •ë³´
    st.caption("â‘¡ ê³„ì¢Œ ì •ë³´")
    df_acct = st.session_state.get("df_acct", pd.DataFrame()).copy()
    if not df_acct.empty:
        col_chart, col_grid = st.columns([1, 1], gap="large")

        with col_chart:
            st.markdown("**ê³„ì¢Œ ìœ í˜•ë³„ í‰ê°€ê¸ˆì•¡ ë¶„í¬**")
            tmp = df_acct.copy()
            tmp["í‰ê°€ì ë¦½ê¸ˆ"] = pd.to_numeric(tmp["í‰ê°€ì ë¦½ê¸ˆ"], errors="coerce").fillna(0)
            grp = tmp.groupby("ê³„ì¢Œ ìœ í˜•", dropna=False)["í‰ê°€ì ë¦½ê¸ˆ"].sum().reset_index().sort_values("í‰ê°€ì ë¦½ê¸ˆ", ascending=True)
            fig = px.bar(grp, x="í‰ê°€ì ë¦½ê¸ˆ", y="ê³„ì¢Œ ìœ í˜•", orientation="h", color="ê³„ì¢Œ ìœ í˜•", text="í‰ê°€ì ë¦½ê¸ˆ")
            fig.update_traces(texttemplate="%{text:,}", textposition="outside")
            fig.update_layout(margin=dict(l=10, r=10, t=10, b=10), legend_title_text="ê³„ì¢Œ ìœ í˜•")
            st.plotly_chart(fig, use_container_width=True)

        with col_grid:
            view_cols = ["ê³„ì¢Œ ë²ˆí˜¸","ê³„ì¢Œ ìœ í˜•","ìƒí’ˆì½”ë“œ","ê°œì„¤ì¼ì","í‰ê°€ì ë¦½ê¸ˆ"]
            aggrid_table(
                df_acct[view_cols].copy(), key=GRID_KEYS["acct"], selection_mode="none", height=320,
                enable_filter=True, fit_columns_on_load=False, allow_horizontal_scroll=True
            )

        st.markdown('<div class="small-note">í•´ë‹¹ ê³ ê°ì˜ ëª¨ë“  ê³„ì¢Œê°€ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ë©ë‹ˆë‹¤. DC ê³„ì•½ì€ DC ìœ í˜• ê³„ì¢Œì— í•œí•´ ì—°ê²°ë©ë‹ˆë‹¤.</div>', unsafe_allow_html=True)
    else:
        st.info("ê³ ê° ì„ íƒ ì‹œ ê³„ì¢Œ ì •ë³´ê°€ í‘œì‹œë©ë‹ˆë‹¤.")

    # â‘¢ DC ê³„ì•½
    st.markdown("---")
    st.caption("â‘¢ DC ê³„ì•½ ì •ë³´")
    df_dc = st.session_state.get("df_dc", pd.DataFrame())
    if df_dc is not None and not df_dc.empty:
        view_cols = ["ê³„ì•½ë²ˆí˜¸","ê·¼ë¬´ì²˜ëª…","ì…ì‚¬ì¼ì","ì¤‘ê°„ì •ì‚°ì¼ì","ì œë„ê°€ì…ì¼ì","ë¶€ë‹´ê¸ˆë‚©ì…ì›ê¸ˆ","ìš´ìš©ì†ìµê¸ˆì•¡","í‰ê°€ì ë¦½ê¸ˆí•©ê³„ê¸ˆì•¡"]
        use_cols = [c for c in view_cols if c in df_dc.columns]
        aggrid_table(
            df_dc[use_cols].copy(), key=GRID_KEYS["dc"], selection_mode="none", height=260,
            enable_filter=True, fit_columns_on_load=False, allow_horizontal_scroll=True
        )
    else:
        st.info("ì„ íƒëœ ê³ ê°ì˜ DC ìœ í˜• ê³„ì¢Œì— ë§¤í•‘ë˜ëŠ” DC ê³„ì•½ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown('</div>', unsafe_allow_html=True)


# -------- MID SEP --------
with midsep:
    st.markdown('<div class="v-sep"></div>', unsafe_allow_html=True)


# -------- RIGHT --------
with right:
    st.markdown('<div class="panel-soft flush-top">', unsafe_allow_html=True)
    st.subheader("ì±—ë´‡ Â· ì‹œë®¬ë ˆì´ì…˜")

    # Debug ëª¨ë“œ í† ê¸€
    debug_on = st.toggle("ë””ë²„ê·¸ ëª¨ë“œ", value=False)

    # 1) (DEBUG ì „ìš©) LLM ì˜µì…˜/ì§„ë‹¨ íŒ¨ë„ â€” Context Builder ê·¸ëŒ€ë¡œ ì¡´ì¬, 4ì»¬ëŸ¼ ì••ì¶• ë°°ì¹˜
    if debug_on:
        with st.expander("âš™ï¸ LLM ì„±ëŠ¥/ì§„ë‹¨ (Debug ì „ìš©)", expanded=False):
            cfg = st.session_state.get("agent_cfg") or {}
            prov = st.selectbox(
                "Provider", options=["ollama", "openai"],
                index=0 if cfg.get("provider", "ollama") == "ollama" else 1
            )

            # ê³µí†µ ê¸°ë³¸ê°’ í—¬í¼
            def _opt(name, default):
                return (cfg.get("options", {}) or {}).get(name, default)

            if prov == "openai":
                # OpenAI ê¸°ë³¸ê°’ ë³´ì • (num_predict < 1 ë°©ì§€)
                raw_np = _opt("num_predict", 1024)
                try:
                    default_np = int(raw_np)
                except Exception:
                    default_np = 1024
                if default_np < 1:
                    default_np = 1024

                c1, c2, c3, c4 = st.columns([1.2, 1, 1, 1])
                with c1:
                    model_id = st.selectbox("OpenAI ëª¨ë¸", options=["gpt-4o-mini"], index=0)
                    temperature = st.number_input("temperature", 0.0, 2.0, float(_opt("temperature", 0.3)), 0.05)
                with c2:
                    top_p = st.number_input("top_p", 0.1, 1.0, float(_opt("top_p", 0.9)), 0.05)
                    num_predict = st.number_input("num_predict (â‰¥1)", min_value=1, max_value=40960, value=default_np)
                with c3:
                    num_thread = st.number_input("num_thread (0=auto)", min_value=0, max_value=32, value=int(_opt("num_thread", 0)))
                    rag_on = st.checkbox("ì§€ì‹ ê²€ìƒ‰ ì‚¬ìš© (RAG)", value=bool(cfg.get("search_knowledge", True)))
                with c4:
                    st.caption("OpenAIëŠ” num_ctx/top_k/repeat_penalty/num_batch ë¯¸ì§€ì›")

                if st.button("ì„¤ì • ì ìš©", use_container_width=True):
                    st.session_state.agent_cfg = {
                        "provider": "openai",
                        "model_id": model_id,
                        "search_knowledge": bool(rag_on),
                        "keep_alive": "2h",
                        "options": {
                            "temperature": float(temperature),
                            "top_p": float(top_p),
                            "num_predict": int(num_predict),     # â‰¥ 1
                            "num_thread": int(num_thread),
                        },
                    }
                    st.session_state.pop("_agent_sig", None)
                    st.success("LLM ì„¤ì •ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.")

            else:
                # OLLAMA (qwen3-h2o2-14b ê¸°ë³¸) â€” 4ì»¬ëŸ¼ x 2í–‰ êµ¬ì„±
                r1c1, r1c2, r1c3, r1c4 = st.columns([1.5, 1, 1, 1])
                with r1c1:
                    model_id = st.text_input("Ollama ëª¨ë¸ ID", value=cfg.get("model_id", "qwen3-h2o2-14b"),
                                            help="ì˜ˆ: qwen3-h2o2-14b")
                with r1c2:
                    num_ctx = st.number_input("num_ctx", min_value=1024, max_value=32768, value=int(_opt("num_ctx", 16384)), step=1024)
                with r1c3:
                    num_predict = st.number_input("num_predict (-1=ë¬´ì œí•œ)", min_value=-1, max_value=40960, value=int(_opt("num_predict", -1)))
                with r1c4:
                    num_batch = st.number_input("num_batch", min_value=1, max_value=8192, value=int(_opt("num_batch", 512)))

                r2c1, r2c2, r2c3, r2c4 = st.columns(4)
                with r2c1:
                    temperature = st.number_input("temperature", 0.0, 2.0, float(_opt("temperature", 0.3)), 0.05)
                with r2c2:
                    top_p = st.number_input("top_p", 0.1, 1.0, float(_opt("top_p", 0.9)), 0.05)
                with r2c3:
                    top_k = st.number_input("top_k", min_value=1, max_value=200, value=int(_opt("top_k", 40)))
                with r2c4:
                    repeat_penalty = st.number_input("repeat_penalty", min_value=0.8, max_value=2.0,
                                                    value=float(_opt("repeat_penalty", 1.05)), step=0.01)

                r3c1, r3c2, _, _ = st.columns(4)
                with r3c1:
                    num_thread = st.number_input("num_thread (0=auto)", min_value=0, max_value=32, value=int(_opt("num_thread", 0)))
                with r3c2:
                    rag_on = st.checkbox("ì§€ì‹ ê²€ìƒ‰ ì‚¬ìš© (RAG)", value=bool(cfg.get("search_knowledge", True)))

                if st.button("ì„¤ì • ì ìš©", use_container_width=True):
                    st.session_state.agent_cfg = {
                        "provider": "ollama",
                        "model_id": model_id,
                        "search_knowledge": bool(rag_on),
                        "keep_alive": "2h",
                        "options": {
                            "num_ctx": int(num_ctx),
                            "num_predict": int(num_predict),      # -1 í—ˆìš©
                            "num_batch": int(num_batch),
                            "num_thread": int(num_thread),
                            "temperature": float(temperature),
                            "top_p": float(top_p),
                            "top_k": int(top_k),
                            "repeat_penalty": float(repeat_penalty),
                        },
                    }
                    st.session_state.pop("_agent_sig", None)
                    st.success("LLM ì„¤ì •ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.")

            # (ì„ íƒ) ê°„ë‹¨ ì§„ë‹¨ ì‹¤í–‰ â€” ë ˆì´ì•„ì›ƒ ì˜í–¥ ìµœì†Œí™” (ìë™ ìŠ¤í¬ë¡¤ ìŠ¤í¬ë¦½íŠ¸ì— ë§¡ê¹€)
            if st.button("ê°„ë‹¨ ì§„ë‹¨ ì‹¤í–‰", use_container_width=True):
                ctx_probe = {"note": "latency probe"}
                prompt_probe = "í•œ ì¤„ë¡œ ëŒ€ë‹µ: ì•ˆë…•í•˜ì„¸ìš”ë¼ê³ ë§Œ ì¶œë ¥."
                diag_container = st.container()
                streamed = ""
                for chunk in run_agent_stream(prompt_probe, ctx_probe, debug=True):
                    streamed += chunk
                    visible, _ = mask_thoughts(streamed, notice_inserted=False)
                    with diag_container:
                        st.markdown(visible, unsafe_allow_html=True)
                t = st.session_state.get("last_debug", {}).get("timing", {})
                st.info(f"TTFT: {t.get('ttft_sec','?')}s / Stream: {t.get('stream_sec','?')}s / Total: {t.get('total_sec','?')}s")

    # 2) === ì»¨í…ìŠ¤íŠ¸/íŒŒë¼ë¯¸í„° ë¹Œë” === (í•­ìƒ í‘œì‹œ, expanded=True ìœ ì§€)
    with st.expander("ì—°ê¸ˆ ì‹œë®¬ë ˆì´ì…˜ íŒŒë¼ë¯¸í„° ë¹Œë” (ì»¨í…ìŠ¤íŠ¸ ë¹Œë”)", expanded=True):
        col_left, col_right = st.columns([1, 1], gap="large")

        with col_left:
            st.markdown("#### ë™ì‘")
            c1, c2 = st.columns(2)
            with c1:
                if st.button("Clear", use_container_width=True, help="ì»¨í…ìŠ¤íŠ¸ë¥¼ ë©”ëª¨ë§Œ ë‚¨ê¸°ê³  ë¹„ì›ë‹ˆë‹¤(ì™¼ìª½ ì„ íƒì€ ìœ ì§€)."):
                    st.session_state.sim_params = DEFAULT_PARAM_SCHEMA.copy()
                    st.session_state.context = {}
                    st.success("ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¹„ì› ìŠµë‹ˆë‹¤(ë©”ëª¨ë§Œ ìœ ì§€).")
            with c2:
                if st.button("Reset", use_container_width=True, help="ì™¼ìª½ í˜„ì¬ ì„ íƒ(ê³ ê°) ê¸°ì¤€ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ë³µì›"):
                    if st.session_state.get("selected_customer"):
                        st.session_state.df_acct = load_accounts_from_db(st.session_state.selected_customer)
                        df_acct_all = st.session_state.df_acct
                        dc_acct_ids = df_acct_all.loc[
                            (df_acct_all["ê³„ì¢Œ ìœ í˜•"] == "DC") & (df_acct_all["_account_id"].notna()),
                            "_account_id"
                        ].astype(str).tolist()
                        st.session_state.df_dc = load_dc_contracts_from_db(dc_acct_ids)
                        st.session_state.context = build_context_from_selection()
                        st.success("ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³µì›í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.session_state.context = {}
                        st.info("ì„ íƒëœ ê³ ê°ì´ ì—†ìŠµë‹ˆë‹¤.")

            st.markdown("#### ìˆ˜ë™ ì¡°ì •")
            p = st.session_state.setdefault("sim_params", DEFAULT_PARAM_SCHEMA.copy())
            p["notes"] = st.text_area("ë©”ëª¨(ì„ íƒ)", value=p.get("notes") or "", height=100)

            # JSON ë³µì‚¬ ë²„íŠ¼
            payload_preview_left = {"params": st.session_state.sim_params, "context": st.session_state.context}
            show_korean_left = st.checkbox("í‘œì‹œìš©(í•œê¸€ ë¼ë²¨)ë¡œ ë³´ê¸°", value=True, key="show_kor_preview_left")
            display_payload_left = koreanize_payload(payload_preview_left) if show_korean_left else payload_preview_left
            json_str_left = to_json_str(display_payload_left)
            _json_for_js = json_str_left.replace("\\", "\\\\").replace("`", "\\`")
            components.v1.html(
                f"""
                <button id="copy-json-btn" style="padding:.5rem .75rem;cursor:pointer;">ğŸ“‹ JSON ë³µì‚¬</button>
                <script>
                  const btn = document.getElementById('copy-json-btn');
                  btn.addEventListener('click', async () => {{
                    try {{
                      await navigator.clipboard.writeText(`{_json_for_js}`);
                      alert('JSONì´ í´ë¦½ë³´ë“œì— ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤.');
                    }} catch (e) {{
                      alert('ë³µì‚¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì½˜ì†”ì„ í™•ì¸í•˜ì„¸ìš”.');
                      console.error(e);
                    }}
                  }});
                </script>
                """,
                height=48,
            )

        with col_right:
            # JSON ë¯¸ë¦¬ë³´ê¸°: ê¸°ë³¸ "ì ‘í˜" ìƒíƒœ
            with st.expander("JSON ë¯¸ë¦¬ë³´ê¸° (ì ‘í˜ ê¸°ë³¸)", expanded=False):
                edit_mode = st.toggle("í¸ì§‘ ëª¨ë“œ(ê³ ê¸‰): ë‚´ë¶€ ìŠ¤í‚¤ë§ˆ JSON ì§ì ‘ ìˆ˜ì •", value=False)
                payload_preview = {"params": st.session_state.sim_params, "context": st.session_state.context}

                if not edit_mode:
                    show_korean = st.checkbox("í‘œì‹œìš©(í•œê¸€ ë¼ë²¨)ë¡œ ë³´ê¸°", value=True, key="show_kor_preview_right")
                    display_payload = koreanize_payload(payload_preview) if show_korean else payload_preview
                    st.json(display_payload)
                else:
                    raw_str = st.text_area("í¸ì§‘(JSON, ì˜ë¬¸ í‚¤)", value=to_json_str(payload_preview), height=260)
                    apply_col1, apply_col2 = st.columns([1, 3])
                    with apply_col1:
                        if st.button("ì ìš©", type="primary"):
                            try:
                                new_payload = json.loads(raw_str)
                                if not isinstance(new_payload, dict):
                                    raise ValueError("payloadëŠ” objectì—¬ì•¼ í•©ë‹ˆë‹¤.")
                                params = new_payload.get("params", {})
                                context = new_payload.get("context", {})
                                if not isinstance(params, dict) or (context is not None and not isinstance(context, dict)):
                                    raise ValueError("params/contextëŠ” objectì—¬ì•¼ í•©ë‹ˆë‹¤.")
                                st.session_state.sim_params = {"notes": params.get("notes", "")}
                                st.session_state.context = context if isinstance(context, dict) else {}
                                st.success("JSONì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.")
                            except Exception as e:
                                st.error(f"ì ìš© ì‹¤íŒ¨: {e}")
                    with apply_col2:
                        st.caption("ì£¼ì˜: ë‚´ë¶€ í‚¤(ì˜ë¬¸) ìŠ¤í‚¤ë§ˆë¡œë§Œ í¸ì§‘ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

    # -------------------- ì±„íŒ… UI --------------------
    st.divider()
    st.markdown("#### ì±„íŒ…")

    # 1) ì±„íŒ… ë©”ì‹œì§€ ì˜ì—­ (ì…ë ¥ì°½ë³´ë‹¤ ìœ„)
    chat_holder = st.container()
    with chat_holder:
        for msg in st.session_state.messages:
            role = "assistant" if msg["role"] == "assistant" else "user"
            st.chat_message(role).markdown(msg["content"], unsafe_allow_html=True)

    # 2) âœ… ì…ë ¥ì°½ 'ë°”ë¡œ ìœ„'ì— ì•µì»¤(ìŠ¤í¬ë¡¤ ëª©í‘œ ì§€ì )
    st.markdown("<div id='right-chat-input-anchor'></div>", unsafe_allow_html=True)

    # 3) ê¸°ë³¸ ì…ë ¥ì°½ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ (ì˜¤ë¥¸ìª½ í•˜ë‹¨)
    user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ DC ê´€ë ¨ ê·œì • ì„¤ëª…)")

    # 4) í˜ì´ì§€ ë Œë” ì§í›„ì—ë„ í•­ìƒ ì•µì»¤ë¡œ ìŠ¤í¬ë¡¤ â†’ ì…ë ¥ì°½ì´ í™”ë©´ì— ë°”ë¡œ ë‚˜íƒ€ë‚¨
    components.v1.html("""
    <script>
    const el = document.getElementById('right-chat-input-anchor');
    if (el) el.scrollIntoView({behavior: 'auto', block: 'end'});
    </script>
    """, height=0)

    # 5) ì „ì†¡ ì²˜ë¦¬: ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì—ë„ ì…ë ¥ì°½ ê·¼ì²˜ë¡œ ìŠ¤í¬ë¡¤ ìœ ì§€
    if user_input:
        # (1) ìœ ì € ë©”ì‹œì§€ ì €ì¥/ë Œë”
        st.session_state.messages.append({"role": "user", "content": user_input})
        with chat_holder:
            st.chat_message("user").markdown(user_input)

            # (2) ì»¨í…ìŠ¤íŠ¸ í™•ì • í›„, ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
            ctx = build_context_for_chat()
            resp_area = st.chat_message("assistant")
            placeholder = resp_area.empty()
            streamed = ""
            displayed_once_think = False

            for chunk in run_agent_stream(user_input, ctx, debug=debug_on):
                streamed += chunk
                visible, displayed_once_think = mask_thoughts(streamed, displayed_once_think)
                placeholder.markdown(visible, unsafe_allow_html=True)

            # (3) ìµœì¢… í…ìŠ¤íŠ¸ ì €ì¥
            final_visible, _ = mask_thoughts(streamed, displayed_once_think)
            st.session_state.messages.append({"role": "assistant", "content": final_visible})


    st.markdown('</div>', unsafe_allow_html=True)  # ì˜¤ë¥¸ìª½ íŒ¨ë„ ë
